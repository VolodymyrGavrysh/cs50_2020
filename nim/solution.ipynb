{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Nim():\n",
    "\n",
    "    def __init__(self, initial=[1, 3, 5, 7]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        Each game board has\n",
    "            - `piles`: a list of how many elements remain in each pile\n",
    "            - `player`: 0 or 1 to indicate which player's turn\n",
    "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
    "        \"\"\"\n",
    "        self.piles = initial.copy()\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, piles):\n",
    "        \"\"\"\n",
    "        Nim.available_actions(piles) takes a `piles` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of removing `j` items\n",
    "        from pile `i` (where piles are 0-indexed).\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i, pile in enumerate(piles):\n",
    "            for j in range(1, pile + 1):\n",
    "                actions.add((i, j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        Nim.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = Nim.other_player(self.player)\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        pile, count = action\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif pile < 0 or pile >= len(self.piles):\n",
    "            raise Exception(\"Invalid pile\")\n",
    "        elif count < 1 or count > self.piles[pile]:\n",
    "            raise Exception(\"Invalid number of objects\")\n",
    "\n",
    "        # Update pile\n",
    "        self.piles[pile] -= count\n",
    "        self.switch_player()\n",
    "\n",
    "        # Check for a winner\n",
    "        if all(pile == 0 for pile in self.piles):\n",
    "            self.winner = self.player\n",
    "\n",
    "\n",
    "class NimAI():\n",
    "\n",
    "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize AI with an empty Q-learning dictionary,\n",
    "        an alpha (learning) rate, and an epsilon rate.\n",
    "\n",
    "        The Q-learning dictionary maps `(state, action)`\n",
    "        pairs to a Q-value (a number).\n",
    "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
    "         - `action` is a tuple `(i, j)` for an action\n",
    "        \"\"\"\n",
    "        self.q = dict()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update Q-learning model, given an old state, an action taken\n",
    "        in that state, a new resulting state, and the reward received\n",
    "        from taking that action.\n",
    "        \"\"\"\n",
    "        old = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old, reward, best_future)\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the Q-value for the state `state` and the action `action`.\n",
    "        If no Q-value exists yet in `self.q`, return 0.\n",
    "        \"\"\"\n",
    "        if self.q[tuple(state), action] not in self.q:\n",
    "            return 0\n",
    "\n",
    "        return self.q[tuple(state), action]\n",
    "\n",
    "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
    "        \"\"\"\n",
    "        Update the Q-value for the state `state` and the action `action`\n",
    "        given the previous Q-value `old_q`, a current reward `reward`,\n",
    "        and an estiamte of future rewards `future_rewards`.\n",
    "\n",
    "        Use the formula:\n",
    "\n",
    "        Q(s, a) <- old value estimate\n",
    "                   + alpha * (new value estimate - old value estimate)\n",
    "\n",
    "        where `old value estimate` is the previous Q-value,\n",
    "        `alpha` is the learning rate, and `new value estimate`\n",
    "        is the sum of the current reward and estimated future rewards.\n",
    "        \"\"\"\n",
    "        self.q[tuple(state), action] = old_q + self.alpha * ((reward + future_rewards) - old_q)\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "        \"\"\"\n",
    "        Given a state `state`, consider all possible `(state, action)`\n",
    "        pairs available in that state and return the maximum of all\n",
    "        of their Q-values.\n",
    "\n",
    "        Use 0 as the Q-value if a `(state, action)` pair has no\n",
    "        Q-value in `self.q`. If there are no available actions in\n",
    "        `state`, return 0.\n",
    "        \"\"\"\n",
    "        best = 0\n",
    "        states = Nim.available_actions(state)\n",
    "        \n",
    "        for action in states:\n",
    "            if self.get_q_value(state, action) > best:\n",
    "                best = self.get_q_value(state, action)\n",
    "                \n",
    "        return best \n",
    "            \n",
    "    def choose_action(self, state, epsilon=True):\n",
    "        \"\"\"\n",
    "        Given a state `state`, return an action `(i, j)` to take.\n",
    "\n",
    "        If `epsilon` is `False`, then return the best action\n",
    "        available in the state (the one with the highest Q-value,\n",
    "        using 0 for pairs that have no Q-values).\n",
    "\n",
    "        If `epsilon` is `True`, then with probability\n",
    "        `self.epsilon` choose a random available action,\n",
    "        otherwise choose the best action available.\n",
    "\n",
    "        If multiple actions have the same Q-value, any of those\n",
    "        options is an acceptable return value.\n",
    "        \"\"\"\n",
    "        actions = Nim.available_actions(state)\n",
    "        best = -100000\n",
    "        best_action = None\n",
    "        \n",
    "        if epsilon and random.random() <= best:\n",
    "            return random.sample(actions, 1)[0]\n",
    "         \n",
    "        for action in actions:\n",
    "            if self.get_q_value(state, action) > best:\n",
    "                best = self.get_q_value(state, action)\n",
    "                best_action = action\n",
    "                \n",
    "        return best_action\n",
    "                \n",
    "def train(n):\n",
    "    \"\"\"\n",
    "    Train an AI by playing `n` games against itself.\n",
    "    \"\"\"\n",
    "\n",
    "    player = NimAI()\n",
    "\n",
    "    # Play n games\n",
    "    for i in range(n):\n",
    "        print(f\"Playing training game {i + 1}\")\n",
    "        game = Nim()\n",
    "\n",
    "        # Keep track of last move made by either player\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "\n",
    "            # Keep track of current state and action\n",
    "            state = game.piles.copy()\n",
    "            action = player.choose_action(game.piles)\n",
    "\n",
    "            # Keep track of last state and action\n",
    "            last[game.player][\"state\"] = state\n",
    "            last[game.player][\"action\"] = action\n",
    "\n",
    "            # Make move\n",
    "            game.move(action)\n",
    "            new_state = game.piles.copy()\n",
    "\n",
    "            # When game is over, update Q values with rewards\n",
    "            if game.winner is not None:\n",
    "                player.update(state, action, new_state, -1)\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    1\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # If game is continuing, no rewards yet\n",
    "            elif last[game.player][\"state\"] is not None:\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    print(\"Done training\")\n",
    "\n",
    "    # Return the trained AI\n",
    "    return player\n",
    "\n",
    "\n",
    "def play(ai, human_player=None):\n",
    "    \"\"\"\n",
    "    Play human game against the AI.\n",
    "    `human_player` can be set to 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no player order set, choose human's order randomly\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Create new game\n",
    "    game = Nim()\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Print contents of piles\n",
    "        print()\n",
    "        print(\"Piles:\")\n",
    "        for i, pile in enumerate(game.piles):\n",
    "            print(f\"Pile {i}: {pile}\")\n",
    "        print()\n",
    "\n",
    "        # Compute available actions\n",
    "        available_actions = Nim.available_actions(game.piles)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Let human make a move\n",
    "        if game.player == human_player:\n",
    "            print(\"Your Turn\")\n",
    "            while True:\n",
    "                pile = int(input(\"Choose Pile: \"))\n",
    "                count = int(input(\"Choose Count: \"))\n",
    "                if (pile, count) in available_actions:\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "\n",
    "        # Have AI make a move\n",
    "        else:\n",
    "            print(\"AI's Turn\")\n",
    "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
    "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
    "\n",
    "        # Make move\n",
    "        game.move((pile, count))\n",
    "\n",
    "        # Check for winner\n",
    "        if game.winner is not None:\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
    "            print(f\"Winner is {winner}\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118065226846859"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://youtu.be/ib18wir5pMk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
